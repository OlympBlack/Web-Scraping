# Projet de Scraping Web (BrainyQuote)

Une application full-stack de web scraping construite avec **Nuxt** (Frontend) et **FastAPI + Playwright** (Backend), int√©gr√©e avec **Supabase** pour la base de donn√©es et le stockage.

## üöÄ Liens de Production

*   **Frontend (Site Web)** : [https://web-scraper-snowy.vercel.app/](https://web-scraper-snowy.vercel.app/)
*   **Backend (API)** : [https://web-scraper-4luz.onrender.com](https://web-scraper-4luz.onrender.com) (Document√© ici pour r√©f√©rence, utilis√© par le frontend)

> [!IMPORTANT]
> **Attention lors du d√©ploiement du Backend** :
> Assurez-vous d'utiliser le **Runtime Docker** sur votre h√©bergeur (Render, Railway, etc.). N'utilisez PAS l'environnement "Python Native" ou "Shell" par d√©faut, car Playwright a besoin de d√©pendances syst√®me sp√©cifiques qui sont incluses dans notre `Dockerfile`.

---

## Fonctionnalit√©s
- **Scraping** : Extraction automatis√©e de citations et d'images depuis BrainyQuote.
- **Retour en temps r√©el** : Barre de progression et mises √† jour de statut en direct via Server-Sent Events (SSE).
- **Stockage** : Les images sont t√©l√©charg√©es, trait√©es et stock√©es dans Supabase Storage avec les types MIME corrects.
- **Base de donn√©es** : Les citations sont sauvegard√©es dans une base de donn√©es Postgres Supabase.

---

## Pr√©requis
- **Python 3.10+**
- **Node.js 18+**
- **Compte Supabase** avec un nouveau projet.

---

## Installation et Configuration (Local)

### 1. Configuration du Backend

Naviguez dans le dossier backend :
```bash
cd backend
```

Cr√©ez un environnement virtuel et installez les d√©pendances :
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate

# Installer les paquets
pip install -r requirements.txt
playwright install chromium
```

Cr√©ez un fichier `.env` dans `backend/` :
```ini
SUPABASE_URL=votre_url_projet_supabase
SUPABASE_KEY=votre_cle_anon_supabase
SUPABASE_SERVICE_ROLE_KEY=votre_cle_service_role
```

### 2. Configuration du Frontend

Naviguez dans le dossier frontend :
```bash
cd ../frontend
```

Installez les d√©pendances :
```bash
npm install
```

### 3. Configuration de Supabase

1.  **Base de donn√©es** : Cr√©ez une table nomm√©e `quotes`.
    ```sql
    create table quotes (
      id bigint generated by default as identity primary key,
      created_at timestamp with time zone default timezone('utc'::text, now()) not null,
      text text,
      author text,
      topic text,
      link text unique,
      image_url text
    );
    ```
2.  **Stockage** : Cr√©ez un bucket public nomm√© `quote-images`.

---

## Utilisation

### Lancer Localement

Vous avez besoin de deux terminaux.

**Terminal 1 : Backend**
```bash
cd backend
venv\Scripts\activate  # Windows
python run.py
```
*Le backend tourne sur http://127.0.0.1:8000*

**Terminal 2 : Frontend**
```bash
cd frontend
npm run dev
```
*Le frontend tourne sur http://localhost:3000*

**Utilisation** : Ouvrez l'URL du frontend, entrez un sujet (ex: "Success"), et cliquez sur "Lancer".

---

## ‚òÅÔ∏è Guide de D√©ploiement Complet

Cette section vous guide pas √† pas pour mettre en ligne votre propre version du Web Scraper.

### üìù Note Importante sur Supabase

Pour que l'application fonctionne correctement et que VOUS puissiez voir les donn√©es r√©colt√©es, **vous devez d√©ployer ce projet en utilisant votre propre compte Supabase**.
*   Si vous utilisez le projet de d√©monstration, vous ne pourrez pas voir les nouvelles entr√©es dans le tableau de bord ni acc√©der au stockage.
*   Cr√©ez un nouveau projet sur [Supabase.com](https://supabase.com/) pour obtenir vos propres URL et Cl√©s API.

---

### 1. D√©ploiement du Backend (API)

Nous recommandons **Render** ou **Railway** pour h√©berger le backend Python.

> [!WARNING]
> **Point Critique : Runtime Docker**
> Ce projet utilise Playwright, qui n√©cessite des navigateurs install√©s dans le syst√®me. Vous **DEVEZ** configurer votre h√©bergeur pour utiliser **Docker** (via le `Dockerfile` fourni) et NON l'environnement Python natif.

**Sur Render / Railway :**
1.  Connectez votre d√©p√¥t GitHub.
2.  **Root Directory** : D√©finissez-le sur `backend` (tr√®s important, sinon le build √©chouera).
3.  **Build Environment** : S√©lectionnez **Docker**.
4.  **Variables d'Environnement** : Ajoutez les cl√©s suivantes (trouv√©es dans vos param√®tres Supabase √† *Project Settings > API*) :
    *   `SUPABASE_URL` : L'URL de votre projet (ex: `https://xyz.supabase.co`).
    *   `SUPABASE_KEY` : Votre cl√© publique `anon`.
    *   `SUPABASE_SERVICE_ROLE_KEY` : Votre cl√© secr√®te `service_role` (n√©cessaire pour contourner les restrictions lors de l'upload).

Une fois d√©ploy√©, copiez l'URL fournie (ex: `https://mon-scraper-backend.onrender.com`).

---

### 2. D√©ploiement du Frontend (Site Web)

Nous recommandons **Vercel** pour le frontend Nuxt.

1.  Allez sur Vercel et importez votre projet GitHub.
2.  **Root Directory** : Cliquez sur "Edit" et s√©lectionnez le dossier `frontend`.
3.  **Variables d'Environnement** :
    *   Nom : `NUXT_PUBLIC_API_BASE`
    *   Valeur : L'URL de votre backend fra√Æchement d√©ploy√© (√©tape 1) **sans le slash √† la fin**.
    *   *Exemple : `https://mon-scraper-backend.onrender.com`*
4.  Lancez le d√©ploiement.

---

### 3. V√©rification Finale (Supabase)

Pour que tout soit op√©rationnel, assurez-vous d'avoir configur√© votre projet Supabase :

1.  **Table Database** : Allez dans l'√©diteur SQL de Supabase et ex√©cutez le script de cr√©ation de table (voir section *Configuration de Supabase* ci-dessus).
2.  **Stockage** :
    *   Cr√©ez un nouveau Bucket nomm√© `quote-images`.
    *   **Important** : Cochez "Public Bucket" lors de la cr√©ation pour que les images soient accessibles par le site web.
    *   Si vous oubliez de le mettre en public, les images s'uploaderont mais ne s'afficheront pas.

üéâ **F√©licitations !** Votre Web Scraper est maintenant enti√®rement d√©ploy√© et fonctionnel.
